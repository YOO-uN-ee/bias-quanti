# Quantifying Bias in Language Models
What matters more? Model architecture? Model size? Training Data?
Tracking down the source/cause of bias that gets propagated through language models.

Models:
- [BERT](https://huggingface.co/google-bert/bert-base-uncased)
- [RoBERTa](https://huggingface.co/FacebookAI/roberta-base)
- [DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased)
- [DeBERTa](https://huggingface.co/microsoft/deberta-v3-base)
- [Electra](https://huggingface.co/google/electra-base-discriminator)
- [XLNet]()
- [ModernBERT](https://huggingface.co/answerdotai/ModernBERT-base)
- [AlBERT](https://huggingface.co/albert/albert-base-v2)

Corpus:
- [BookCorpus](https://huggingface.co/datasets/bookcorpus/bookcorpus)
- [Wikipedia](https://huggingface.co/datasets/legacy-datasets/wikipedia)
- [CommonCrawl](https://github.com/allenai/allennlp/discussions/5056)
    - [CCNews (2016--2024)](https://huggingface.co/datasets/stanford-oval/ccnews)
    - Roberta: September 2016 and February 2019
- [Gigawords](https://huggingface.co/datasets/Harvard/gigaword/discussions)
    - https://www.tensorflow.org/datasets/catalog/gigaword
- Clueweb
    - Cannot use due to access issue
- [OpenWebText](https://github.com/jcpeterson/openwebtext) [HFOpenWebText](https://huggingface.co/datasets/Skylion007/openwebtext)
- [Stories](https://huggingface.co/datasets/spacemanidol/cc-stories/tree/main)